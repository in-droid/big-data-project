{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f45c3-4841-4625-a9d0-c544bb9f9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed, compute\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "BASE_DIR = \"/d/hpc/projects/FRI/bigdata/students/in7357/optimized_parquet\"\n",
    "DATASETS = [\"FHVHV\"]\n",
    "CLEANED_BASE = \"/d/hpc/projects/FRI/bigdata/students/in7357/cleaned_parquet\"\n",
    "QUARANTINE_BASE = \"/d/hpc/projects/FRI/bigdata/students/in7357/quarantine_parquet\"\n",
    "OUT_DIR = \"/d/hpc/projects/FRI/bigdata/students/in7357/dq_results_task2\"\n",
    "os.makedirs(CLEANED_BASE, exist_ok=True)\n",
    "os.makedirs(QUARANTINE_BASE, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "MIN_YEAR = 2019\n",
    "MAX_YEAR = 2026\n",
    "MAX_SUSPECT_SPEED_MPH = 120.0\n",
    "MAX_TRIP_HOURS = 24.0\n",
    "DROP_BAD_ROWS = False\n",
    "\n",
    "PICKUP_CANDIDATES = [\n",
    "    \"tpep_pickup_datetime\", \"lpep_pickup_datetime\", \"pickup_datetime\",\n",
    "    \"pickup_time\", \"pickup_ts\", \"pickup\", \"request_datetime\", \"on_scene_datetime\"\n",
    "]\n",
    "DROPOFF_CANDIDATES = [\n",
    "    \"tpep_dropoff_datetime\", \"lpep_dropoff_datetime\", \"dropoff_datetime\",\n",
    "    \"dropoff_time\", \"dropoff_ts\", \"dropoff\"\n",
    "]\n",
    "\n",
    "def detect_col(cols: List[str], candidates: List[str]) -> Optional[str]:\n",
    "    cols_map = {c.lower(): c for c in cols}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_map:\n",
    "            return cols_map[cand.lower()]\n",
    "    for lc, original in cols_map.items():\n",
    "        if 'pickup' in lc and ('date' in lc or 'time' in lc):\n",
    "            return original\n",
    "        if 'dropoff' in lc and ('date' in lc or 'time' in lc):\n",
    "            return original\n",
    "    return None\n",
    "\n",
    "def has_meaningful_values(ddf, col, min_nonnull=1):\n",
    "    if col is None or col not in ddf.columns:\n",
    "        return False\n",
    "    try:\n",
    "        nonnull_count = int(ddf[col].notna().sum().compute())\n",
    "        return nonnull_count >= min_nonnull\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "@delayed\n",
    "def process_file(file_path, dataset):\n",
    "    df = dd.read_parquet(file_path, engine=\"pyarrow\", infer_divisions=False)\n",
    "    rename_map = {c: c.lower() for c in df.columns}\n",
    "    df = df.rename(columns=rename_map)\n",
    "    pickup_col = detect_col(list(df.columns), PICKUP_CANDIDATES)\n",
    "    dropoff_col = detect_col(list(df.columns), DROPOFF_CANDIDATES)\n",
    "    if pickup_col is None:\n",
    "        df['pickup_datetime'] = dd.from_pandas(pd.Series([pd.NaT] * len(df.index.compute())), npartitions=df.npartitions)\n",
    "        pickup_col = 'pickup_datetime'\n",
    "    else:\n",
    "        pickup_col = pickup_col.lower()\n",
    "    if dropoff_col is None:\n",
    "        df['dropoff_datetime'] = dd.to_datetime(df.get('dropoff_datetime'), errors='coerce')\n",
    "        dropoff_col = 'dropoff_datetime'\n",
    "    else:\n",
    "        dropoff_col = dropoff_col.lower()\n",
    "    df[pickup_col] = dd.to_datetime(df[pickup_col], errors='coerce', infer_datetime_format=True).astype('datetime64[us]')\n",
    "    df[dropoff_col] = dd.to_datetime(df[dropoff_col], errors='coerce', infer_datetime_format=True).astype('datetime64[us]')\n",
    "    if 'year' not in df.columns:\n",
    "        df['year'] = df[pickup_col].dt.year.astype('Int64')\n",
    "    df['year'] = dd.to_numeric(df['year'], errors=\"coerce\")\n",
    "    df['_duration_s'] = (df[dropoff_col] - df[pickup_col]).dt.total_seconds()\n",
    "    dist_col = next((c for c in ['trip_distance','trip_miles','trip_mile'] if c in df.columns), None)\n",
    "    pcol = next((c for c in ['passenger_count','passengers'] if c in df.columns), None)\n",
    "    fare_col = next((c for c in df.columns if 'fare' in c), None)\n",
    "    total_col = 'total_amount' if 'total_amount' in df.columns else None\n",
    "    tolls_col = next((c for c in df.columns if 'toll' in c), None)\n",
    "    df['_is_missing_pickup'] = df[pickup_col].isna()\n",
    "    df['_year_out_of_range'] = (~df['year'].isna()) & ((df['year'] < MIN_YEAR) | (df['year'] > MAX_YEAR))\n",
    "    df['_pickup_eq_dropoff'] = (~df[pickup_col].isna()) & (df[dropoff_col] == df[pickup_col])\n",
    "    df['_dropoff_before_pickup'] = (~df[pickup_col].isna()) & (~df[dropoff_col].isna()) & (df[dropoff_col] < df[pickup_col])\n",
    "    df['_trip_duration_zero'] = df['_duration_s'] == 0\n",
    "    df['_trip_duration_negative'] = df['_duration_s'] < 0\n",
    "    if dist_col and has_meaningful_values(df, dist_col):\n",
    "        df['_trip_distance_zero'] = dd.to_numeric(df[dist_col], errors='coerce').fillna(0) == 0\n",
    "        hours = df['_duration_s'] / 3600.0\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            speed = dd.to_numeric(df[dist_col], errors='coerce') / hours\n",
    "        df['_suspicious_speed'] = (hours > 0) & (speed > MAX_SUSPECT_SPEED_MPH)\n",
    "    else:\n",
    "        df['_trip_distance_zero'] = False\n",
    "        df['_suspicious_speed'] = False\n",
    "    if pcol and has_meaningful_values(df, pcol):\n",
    "        pc_num = dd.to_numeric(df[pcol], errors='coerce')\n",
    "        df['_passenger_count_invalid'] = pc_num.isna() | (pc_num < 0) | (pc_num > 10)\n",
    "    else:\n",
    "        df['_passenger_count_invalid'] = False\n",
    "    if fare_col and has_meaningful_values(df, fare_col):\n",
    "        df['_negative_fare'] = dd.to_numeric(df[fare_col], errors='coerce') < 0\n",
    "    else:\n",
    "        df['_negative_fare'] = False\n",
    "    if tolls_col and has_meaningful_values(df, tolls_col):\n",
    "        df['_negative_tolls'] = dd.to_numeric(df[tolls_col], errors='coerce') < 0\n",
    "    else:\n",
    "        df['_negative_tolls'] = False\n",
    "    if total_col and has_meaningful_values(df, total_col):\n",
    "        df['_negative_total'] = dd.to_numeric(df[total_col], errors='coerce') < 0\n",
    "    else:\n",
    "        df['_negative_total'] = False\n",
    "    issue_mask = (\n",
    "        df['_is_missing_pickup'] | df['_year_out_of_range'] | df['_pickup_eq_dropoff'] |\n",
    "        df['_dropoff_before_pickup'] | df['_trip_distance_zero'] | df['_trip_duration_zero'] |\n",
    "        df['_trip_duration_negative'] | df['_passenger_count_invalid'] | df['_negative_fare'] |\n",
    "        df['_negative_tolls'] | df['_negative_total'] | df['_suspicious_speed']\n",
    "    )\n",
    "    agg_cols = [\n",
    "        '_is_missing_pickup','_year_out_of_range','_pickup_eq_dropoff','_dropoff_before_pickup',\n",
    "        '_trip_distance_zero','_trip_duration_zero','_trip_duration_negative',\n",
    "        '_passenger_count_invalid','_negative_fare','_negative_tolls','_negative_total','_suspicious_speed'\n",
    "    ]\n",
    "    grouped = df.groupby('year')[agg_cols].sum().compute()\n",
    "    total_by_year = df.groupby('year').size().compute().rename('total_rows')\n",
    "    summary = grouped.join(total_by_year).reset_index().rename(columns={'year':'pickup_year'})\n",
    "    for col in agg_cols:\n",
    "        summary[col+\"_pct\"] = (summary[col] / summary['total_rows']).fillna(0) * 100.0\n",
    "    return summary, df, issue_mask\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    src_dir = os.path.join(BASE_DIR, dataset)\n",
    "    files = glob.glob(os.path.join(src_dir, \"*.parquet\"))\n",
    "    delayed_results = [process_file(f, dataset) for f in files]\n",
    "    computed = compute(*delayed_results)\n",
    "    all_summaries, dfs, masks = zip(*computed)\n",
    "    combined_summary = pd.concat(all_summaries, ignore_index=True)\n",
    "    csv_out = os.path.join(OUT_DIR, f\"{dataset}_dq_summary_by_year.csv\")\n",
    "    combined_summary.to_csv(csv_out, index=False)\n",
    "    summary_sorted = combined_summary.sort_values('pickup_year')\n",
    "    pct_cols = [c for c in summary_sorted.columns if c.endswith('_pct') and c.startswith('_')]\n",
    "    summary_sorted['year_label'] = summary_sorted['pickup_year'].apply(lambda x: 'unknown' if pd.isna(x) else str(int(x)))\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    bottom = np.zeros(len(summary_sorted))\n",
    "    x = np.arange(len(summary_sorted))\n",
    "    for c in pct_cols:\n",
    "        vals = summary_sorted[c].to_numpy()\n",
    "        ax.bar(x, vals, bottom=bottom, label=c.replace('_pct','').lstrip('_'))\n",
    "        bottom += vals\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(summary_sorted['year_label'], rotation=45)\n",
    "    ax.set_ylabel(\"Percent of rows with issue (%)\")\n",
    "    ax.set_title(f\"{dataset} â€” Data Quality issues by pickup year\")\n",
    "    ax.legend(bbox_to_anchor=(1.02,1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    png_out = os.path.join(OUT_DIR, f\"{dataset}_dq_summary_by_year.png\")\n",
    "    fig.savefig(png_out, dpi=150)\n",
    "    plt.close(fig)\n",
    "    combined_df = dd.concat(dfs)\n",
    "    combined_mask = dd.concat(masks)\n",
    "    clean_df_to_write = combined_df[~combined_mask]\n",
    "    bad_df_to_write = combined_df[combined_mask]\n",
    "    cleaned_out = os.path.join(CLEANED_BASE, dataset)\n",
    "    quarantine_out = os.path.join(QUARANTINE_BASE, dataset)\n",
    "    os.makedirs(cleaned_out, exist_ok=True)\n",
    "    os.makedirs(quarantine_out, exist_ok=True)\n",
    "    clean_df_to_write.to_parquet(cleaned_out, engine=\"pyarrow\", write_index=False, partition_on=['year'], compression=\"snappy\", overwrite=True)\n",
    "    if not DROP_BAD_ROWS:\n",
    "        bad_df_to_write.to_parquet(quarantine_out, engine=\"pyarrow\", write_index=False, partition_on=['year'], compression=\"snappy\", overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f77b8c-5049-4e43-9873-fba4ee12aa4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (big_data311)",
   "language": "python",
   "name": "big_data311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
