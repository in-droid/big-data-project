{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c30836",
   "metadata": {},
   "source": [
    "##  Adding weather features to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d048d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "# Load your taxi data\n",
    "\n",
    "WEATHER_DATA_PATH = \"/d/hpc/projects/FRI/bigdata/students/in7357/weather_data.csv\"\n",
    "OUTPUT_DIR = '/d/hpc/projects/FRI/bigdata/students/in7357/out'\n",
    "CLEANED_DATA = '/d/hpc/projects/FRI/bigdata/students/in7357/cleaned_parquet'\n",
    "yellow_taxi_df = dd.read_parquet(f\"{CLEANED_DATA}/YELLOW/\", engine=\"pyarrow\")\n",
    "\n",
    "yellow_taxi_df[\"pickup_hour\"] = yellow_taxi_df[\"pickup_datetime\"].dt.floor(\"H\")\n",
    "\n",
    "green_taxi_df = dd.read_parquet(f\"{CLEANED_DATA}/GREEN/\", engine=\"pyarrow\")\n",
    "green_taxi_df[\"pickup_hour\"] = green_taxi_df[\"pickup_datetime\"].dt.floor(\"H\")\n",
    "\n",
    "fhvhv_df = dd.read_parquet(f\"{CLEANED_DATA}/FHVH/\", engine=\"pyarrow\")\n",
    "fhvhv_df[\"pickup_hour\"] = fhvhv_df[\"pickup_datetime\"].dt.floor(\"H\")\n",
    "\n",
    "fhv_df = dd.read_parquet(f\"{CLEANED_DATA}/FHV/\", engine=\"pyarrow\")\n",
    "fhv_df[\"pickup_hour\"] = fhv_df[\"pickup_datetime\"].dt.floor(\"H\")\n",
    "\n",
    "weather_df = dd.read_csv(\n",
    "    WEATHER_DATA_PATH,\n",
    "    parse_dates=[\"time\"]\n",
    ")\n",
    "\n",
    "weather_df = weather_df.rename(columns={\"time\": \"pickup_hour\"})\n",
    "\n",
    "weather_df = weather_df.rename(columns=lambda x: x.strip().lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"Â°\", \"deg\"))\n",
    "\n",
    "augmented_yellow_df = dd.merge(\n",
    "    yellow_taxi_df,\n",
    "    weather_df,\n",
    "    on=\"pickup_hour\",\n",
    "    how=\"left\"\n",
    ")\n",
    "augmented_green_df = dd.merge(\n",
    "    green_taxi_df,\n",
    "    weather_df,\n",
    "    on=\"pickup_hour\",\n",
    "    how=\"left\"\n",
    ")\n",
    "augmented_fhvhv_df = dd.merge(\n",
    "    fhvhv_df,\n",
    "    weather_df,\n",
    "    on=\"pickup_hour\",\n",
    "    how=\"left\"\n",
    ")\n",
    "augmented_fhv_df = dd.merge(\n",
    "    fhv_df,\n",
    "    weather_df,\n",
    "    on=\"pickup_hour\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "augmented_yellow_df.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/YELLOW/augmented_with_weather\",\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "augmented_green_df.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/GREEN/augmented_with_weather\",\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "augmented_fhvhv_df.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/FHVH/augmented_with_weather\",\n",
    "    write_index=False\n",
    ")\n",
    "\n",
    "augmented_fhv_df.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/FHV/augmented_with_weather\",\n",
    "    write_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b0e313",
   "metadata": {},
   "source": [
    "## Making new columns from school locations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ddf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "zip_path = \"/d/hpc/projects/FRI/bigdata/students/in7357/taxi_zones.zip\"  \n",
    "extract_dir = \"/d/hpc/projects/FRI/bigdata/students/in7357/taxi_zones_shapefile\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "shp_file = [f for f in os.listdir(extract_dir) if f.endswith(\".shp\")][0]\n",
    "gdf = gpd.read_file(os.path.join(extract_dir, shp_file)).to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Compute centroids\n",
    "gdf[\"latitude\"] = gdf.centroid.y\n",
    "gdf[\"longitude\"] = gdf.centroid.x\n",
    "zone_coords = gdf[[\"LocationID\", \"latitude\", \"longitude\"]].copy()\n",
    "\n",
    "\n",
    "pickup_zones = zone_coords.rename(columns={\n",
    "    \"LocationID\": \"pulocationid\",\n",
    "    \"latitude\": \"pickup_latitude\",\n",
    "    \"longitude\": \"pickup_longitude\"\n",
    "})\n",
    "dropoff_zones = zone_coords.rename(columns={\n",
    "    \"LocationID\": \"dolocationid\",\n",
    "    \"latitude\": \"dropoff_latitude\",\n",
    "    \"longitude\": \"dropoff_longitude\"\n",
    "})\n",
    "\n",
    "\n",
    "yellow_taxi_df['pulocationid'] = yellow_taxi_df['pulocationid'].astype('int32')\n",
    "yellow_taxi_df['dolocationid'] = yellow_taxi_df['dolocationid'].astype('int32')\n",
    "# Merge pickup and dropoff coordinates\n",
    "yellow_taxi_df = yellow_taxi_df.merge(pickup_zones, on=\"pulocationid\", how=\"left\")\n",
    "yellow_taxi_df = yellow_taxi_df.merge(dropoff_zones, on=\"dolocationid\", how=\"left\")\n",
    "\n",
    "green_taxi_df['pulocationid'] = green_taxi_df['pulocationid'].astype('int32')\n",
    "green_taxi_df['dolocationid'] = green_taxi_df['dolocationid'].\n",
    "\n",
    "\n",
    "green_taxi_df = green_taxi_df.merge(pickup_zones, on=\"pulocationid\", how=\"left\")\n",
    "green_taxi_df = green_taxi_df.merge(dropoff_zones, on=\"dolocationid\", how=\"left\")\n",
    "fhvhv_df['pulocationid'] = fhvhv_df['pulocationid'].astype('int32')\n",
    "fhvhv_df['dolocationid'] = fhvhv_df['dolocationid'].astype('int32')\n",
    "fhvhv_df = fhvhv_df.merge(pickup_zones, on=\"pulocationid\", how=\"left\")\n",
    "fhvhv_df = fhvhv_df.merge(dropoff_zones, on=\"dolocationid\", how=\"left\")\n",
    "fhv_df['pulocationid'] = fhv_df['pulocationid'].astype('int32')\n",
    "fhv_df['dolocationid'] = fhv_df['dolocationid'].astype('int32')\n",
    "fhv_df = fhv_df.merge(pickup_zones, on=\"pulocationid\", how=\"left\")\n",
    "fhv_df = fhv_df.merge(dropoff_zones, on=\"dolocationid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a327b387",
   "metadata": {},
   "source": [
    "### Load school locations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_df = pd.read_csv(\"/d/hpc/projects/FRI/bigdata/students/in7357/school_locations.csv\")\n",
    "\n",
    "def extract_coords(location_str):\n",
    "    try:\n",
    "        coordinates = location_str.split('\\n')[-1].strip('(').strip(')').split(',')\n",
    "        lat, long =  map(float, coordinates)\n",
    "        return pd.Series([lat, long])\n",
    "    except Exception:\n",
    "        return pd.Series([None, None])\n",
    "    \n",
    "\n",
    "# def extract_coords(location_str):\n",
    "#     import re\n",
    "#     match = re.search(r'\\\\(([-\\\\d.]+), ([-\\\\d.]+)\\\\)', str(location_str))\n",
    "#     return pd.Series((float(match[1]), float(match[2]))) if match else pd.Series((None, None))\n",
    "\n",
    "schools_df[['school_lat', 'school_lon']] = schools_df['Location 1'].apply(extract_coords)\n",
    "# schools_df = schools_df.dropna(subset=['school_lat', 'school_lon'])\n",
    "\n",
    "school_lat = schools_df['school_lat'].values\n",
    "school_lon = schools_df['school_lon'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd2eb0",
   "metadata": {},
   "source": [
    "### Define Haversine distatance function and feature logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a545b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True)\n",
    "def haversine_numba(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Numba-optimized haversine distance calculation for single point vs array\n",
    "    Returns distances in km\n",
    "    \"\"\"\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1)\n",
    "    lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2)\n",
    "    lon2 = np.radians(lon2)\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    return R * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def enrich_with_school_features(df, school_lat, school_lon):\n",
    "    if df.empty or len(school_lat) == 0:\n",
    "        return df.assign(\n",
    "            pickup_nearest_school_distance_km=np.nan,\n",
    "            pickup_near_school=0,\n",
    "            pickup_school_count_500m=0,\n",
    "            dropoff_near_school=0\n",
    "        )\n",
    "    \n",
    "    # Convert to numpy arrays once\n",
    "    lat_pick = df['pickup_latitude'].values\n",
    "    lon_pick = df['pickup_longitude'].values\n",
    "    lat_drop = df['dropoff_latitude'].values\n",
    "    lon_drop = df['dropoff_longitude'].values\n",
    "    \n",
    "    # Pre-allocate result arrays\n",
    "    n = len(df)\n",
    "    pickup_min_dist = np.full(n, np.inf)\n",
    "    pickup_count_500m = np.zeros(n, dtype=np.int32)\n",
    "    dropoff_min_dist = np.full(n, np.inf)\n",
    "    \n",
    "    # Process schools in batches to control memory usage\n",
    "    batch_size = 100  # Adjust based on available memory\n",
    "    n_schools = len(school_lat)\n",
    "    \n",
    "    for i in range(0, n_schools, batch_size):\n",
    "        batch_end = min(i + batch_size, n_schools)\n",
    "        batch_lat = school_lat[i:batch_end]\n",
    "        batch_lon = school_lon[i:batch_end]\n",
    "        \n",
    "        # Process pickup locations\n",
    "        for j in range(len(batch_lat)):\n",
    "            dists = haversine_numba(lat_pick, lon_pick, batch_lat[j], batch_lon[j])\n",
    "            # Use np.fmin to handle NaN values properly \n",
    "            pickup_min_dist = np.fmin(pickup_min_dist, dists)\n",
    "            pickup_min_dist = np.where(pickup_min_dist == np.inf, np.nan, pickup_min_dist)\n",
    "            # Replace any NaN values with original distance\n",
    "            # pickup_min_dist = np.nan_to_num(pickup_min_dist, nan=np.inf)\n",
    "            \n",
    "            pickup_count_500m += (dists <= 0.5)\n",
    "            \n",
    "            # Process dropoff locations\n",
    "            dists = haversine_numba(lat_drop, lon_drop, batch_lat[j], batch_lon[j])\n",
    "            dropoff_min_dist = np.fmin(dropoff_min_dist, dists)\n",
    "            # cast inf to NaN\n",
    "            dropoff_min_dist = np.where(dropoff_min_dist == np.inf, np.nan, dropoff_min_dist)\n",
    "            # dropoff_min_dist = np.nan_to_num(dropoff_min_dist, nan=np.inf)\n",
    "            # dropoff_min_dist = np.minimum(np.nanmin(dropoff_min_dist), np.nanmin(dists))\n",
    "    \n",
    "    return df.assign(\n",
    "        pickup_nearest_school_distance_km=pickup_min_dist,\n",
    "        pickup_near_school=(pickup_min_dist <= 0.2).astype(float),\n",
    "        pickup_school_count_500m=pickup_count_500m,\n",
    "        dropoff_nearest_school_distance_km=dropoff_min_dist,\n",
    "        dropoff_near_school=(dropoff_min_dist <= 0.2).astype(float)\n",
    "    )\n",
    "\n",
    "result_yellow = enrich_with_school_features(\n",
    "    yellow_taxi_df,\n",
    "    school_lat,\n",
    "    school_lon\n",
    ")\n",
    "\n",
    "result_green = enrich_with_school_features(\n",
    "    green_taxi_df,\n",
    "    school_lat,\n",
    "    school_lon\n",
    ")\n",
    "\n",
    "result_fhvhv = enrich_with_school_features(\n",
    "    fhvhv_df,\n",
    "    school_lat,\n",
    "    school_lon\n",
    ")\n",
    "\n",
    "result_fhv = enrich_with_school_features(\n",
    "    fhv_df,\n",
    "    school_lat,\n",
    "    school_lon\n",
    ")\n",
    "result_yellow.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/YELLOW/augmented_with_school_features\",\n",
    "    write_index=False\n",
    ")\n",
    "result_green.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/GREEN/augmented_with_school_features\",\n",
    "    write_index=False\n",
    ")\n",
    "result_fhvhv.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/FHVH/augmented_with_school_features\",\n",
    "    write_index=False\n",
    ")\n",
    "result_fhv.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/FHV/augmented_with_school_features\",\n",
    "    write_index=False\n",
    ")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad3c674",
   "metadata": {},
   "source": [
    "# Enrich with events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = dd.read_csv('/d/hpc/projects/FRI/bigdata/students/in7357/nyc_event_data', assume_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf78598",
   "metadata": {},
   "outputs": [],
   "source": [
    "events['Start Date/Time'] = dd.to_datetime(events['Start Date/Time'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "events['End Date/Time'] = dd.to_datetime(events['End Date/Time'], format='%m/%d/%Y %I:%M:%S %p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ae4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_iterval = (events['Start Date/Time'] > '2012-01-01') & (events['Start Date/Time'] < '2025-05-01')\n",
    "events = events[selected_iterval]\n",
    "important_event_types = [\n",
    "    \"Marathon\", \n",
    "    \"BID Multi-Block\", \n",
    "    \"Rally\", \n",
    "    # \"Parade\", \n",
    "    # \"Plaza Event\", \n",
    "    \"Festival\", \n",
    "    \"Health Fair\", \n",
    "    # \"Parade\"\n",
    "]\n",
    "# Filter out rows based on Event Name\n",
    "events = events[events['Event Type'].isin(important_event_types)]\n",
    "\n",
    "events['Event Type'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fdcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n",
    "\n",
    "# Initialize Nominatim geocoder\n",
    "# slow processing, so we set a long timeout and rate limit\n",
    "geolocator = Nominatim(user_agent=\"event_location_geocoder\", timeout=100)\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=120)\n",
    "\n",
    "def preprocess_location(location):\n",
    "    # Clean and format the location string\n",
    "    location = (\n",
    "        location.replace(\"between\", \"&\")\n",
    "        .replace(\" and \", \" & \")\n",
    "        .replace(\"STREET\", \"St\")\n",
    "        .replace(\"BOULEVARD\", \"Blvd\")\n",
    "        .replace(\"BROADWAY\", \"Broadway\")\n",
    "        + \", New York, NY\"  # Add city/state for better accuracy\n",
    "    )\n",
    "    return location\n",
    "\n",
    "def get_coordinates(location):\n",
    "    try:\n",
    "        location = preprocess_location(location)\n",
    "        result = geocode(location)\n",
    "        if result:\n",
    "            return (result.latitude, result.longitude)\n",
    "        else:\n",
    "            return (None, None)\n",
    "    except (GeocoderTimedOut, GeocoderUnavailable):\n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "location = \"GOUVERNEUR STREET\"\n",
    "print(get_coordinates(location)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c91ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_locations = events['Event Location'].unique()\n",
    "\n",
    "locations_df = pd.DataFrame(unique_locations, columns=['Event Location'])\n",
    "locations_df[['lat', 'lon']] = locations_df['Event Location'].apply(\n",
    "    lambda x: pd.Series(get_coordinates(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge events with the locations\n",
    "\n",
    "events_merged = events.merge(locations_df, on='Event Location', how='left')\n",
    "events_merged = events_merged.dropna(subset=['lat', 'lon'])\n",
    "\n",
    "events_merged_gdf = gpd.GeoDataFrame(\n",
    "    events_merged,\n",
    "    geometry=gpd.points_from_xy(events_merged['lon'], events_merged['lat']),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(\"EPSG:32618\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with taxi data\n",
    "from curses import meta\n",
    "\n",
    "\n",
    "def process_partition(df_partition, events_gdf):\n",
    "    # Convert partition to pandas DataFrame\n",
    "    df = df_partition.copy()\n",
    "    \n",
    "    pickup_gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df.pickup_longitude, df.pickup_latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:32618\")\n",
    "    \n",
    "    pickup_gdf['buffer'] = pickup_gdf.geometry.buffer(1000)\n",
    "    pickup_joined = gpd.sjoin(\n",
    "        pickup_gdf.set_geometry('buffer'), \n",
    "        events_gdf, \n",
    "        how='inner', \n",
    "        predicate='intersects'\n",
    "    )\n",
    "    pickup_events = pickup_joined[\n",
    "        pickup_joined.tpep_pickup_datetime.between(\n",
    "            pickup_joined['Start Date/Time'], \n",
    "            pickup_joined['End Date/Time']\n",
    "        )\n",
    "    ].groupby(level=0).size().rename('pickup_events')\n",
    "    \n",
    "    # Process dropoff events\n",
    "    dropoff_gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df.dropoff_longitude, df.dropoff_latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:32618\")\n",
    "    \n",
    "    dropoff_gdf['buffer'] = dropoff_gdf.geometry.buffer(1000)\n",
    "    dropoff_joined = gpd.sjoin(\n",
    "        dropoff_gdf.set_geometry('buffer'), \n",
    "        events_gdf, \n",
    "        how='inner', \n",
    "        predicate='intersects'\n",
    "    )\n",
    "    dropoff_events = dropoff_joined[\n",
    "        dropoff_joined.tpep_dropoff_datetime.between(\n",
    "            dropoff_joined['Start Date/Time'], \n",
    "            dropoff_joined['End Date/Time']\n",
    "        )\n",
    "    ].groupby(level=0).size().rename('dropoff_events')\n",
    "    \n",
    "    # Merge results\n",
    "    return df.join(pickup_events, how='left') \\\n",
    "             .join(dropoff_events, how='left') \\\n",
    "             .fillna({'pickup_events': 0, 'dropoff_events': 0}) \\\n",
    "             .assign(total_events=lambda x: x.pickup_events + x.dropoff_events)\n",
    "    \n",
    "# Apply processing to all partitions\n",
    "meta_yellow = yellow_taxi_df._meta.copy()\n",
    "meta_yellow['pickup_events'] = pd.Series(dtype='int64')\n",
    "meta_yellow['dropoff_events'] = pd.Series(dtype='int64')\n",
    "meta_yellow['total_events'] = pd.Series(dtype='int64')\n",
    "\n",
    "\n",
    "meta_green = green_taxi_df._meta.copy()\n",
    "meta_green['pickup_events'] = pd.Series(dtype='int64')\n",
    "meta_green['dropoff_events'] = pd.Series(dtype='int64')\n",
    "meta_green['total_events'] = pd.Series(dtype='int64')\n",
    "\n",
    "meta_fhv = fhv_df._meta.copy()\n",
    "meta_fhv['pickup_events'] = pd.Series(dtype='int64')\n",
    "meta_fhv['dropoff_events'] = pd.Series(dtype='int64')\n",
    "meta_fhv['total_events'] = pd.Series(dtype='int64')\n",
    "\n",
    "meta_fhvgh = fhvhv_df._meta.copy()\n",
    "meta_fhvgh['pickup_events'] = pd.Series(dtype='int64')\n",
    "meta_fhvgh['dropoff_events'] = pd.Series(dtype='int64')\n",
    "meta_fhvgh['total_events'] = pd.Series(dtype='int64')   \n",
    "\n",
    "yellow_taxi_df_location_enriched = yellow_taxi_df.map_partitions(\n",
    "    process_partition,\n",
    "    events_gdf=events_merged_gdf,\n",
    "    meta=meta_yellow\n",
    ")\n",
    "green_taxi_df_location_enriched = green_taxi_df.map_partitions(\n",
    "    process_partition,\n",
    "    events_gdf=events_merged_gdf,\n",
    "    meta=meta_green\n",
    ")\n",
    "fhvhv_df_location_enriched = fhvhv_df.map_partitions(\n",
    "    process_partition,\n",
    "    events_gdf=events_merged_gdf,\n",
    "    meta=meta_fhvgh\n",
    ")\n",
    "fhv_df_location_enriched = fhv_df.map_partitions(\n",
    "    process_partition,\n",
    "    events_gdf=events_merged_gdf,\n",
    "    meta=meta_fhv\n",
    ")\n",
    "\n",
    "\n",
    "yellow_taxi_df_location_enriched.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/YELLOW/augmented_with_events\",\n",
    "    write_index=False\n",
    ")\n",
    "green_taxi_df_location_enriched.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/GREEN/augmented_with_events\",\n",
    "    write_index=False\n",
    ")\n",
    "fhvhv_df_location_enriched.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/FHVH/augmented_with_events\",\n",
    "    write_index=False\n",
    ")   \n",
    "\n",
    "fhv_df_location_enriched.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/FHV/augmented_with_events\",\n",
    "    write_index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae7129e",
   "metadata": {},
   "source": [
    "# Vicinity of major businesses and attractions (based on pickup/dropoff date-time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = pd.read_csv('/d/hpc/projects/FRI/bigdata/students/in7357/DCA_Legally_Operating_Businesses_geocoded_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea735bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = businesses.loc[businesses['License Expiration Date'] > '2012-01-01']\n",
    "attractions = pd.read_excel('/d/hpc/projects/FRI/bigdata/students/in7357/New_York_Tourist_Locations.xlsx')\n",
    "\n",
    "\n",
    "attractions[['lat', 'lon']] = attractions['Address'].apply(\n",
    "    lambda x: pd.Series(get_coordinates(x)))\n",
    "\n",
    "\n",
    "businesses_gdf = gpd.GeoDataFrame(\n",
    "        businesses,\n",
    "        geometry=gpd.points_from_xy(businesses.Longitude, businesses.Latitude),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:32618\") \n",
    "\n",
    "attractions_gdf = gpd.GeoDataFrame(\n",
    "        attractions,\n",
    "        geometry=gpd.points_from_xy(attractions.lon, attractions.lat),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:32618\")\n",
    "\n",
    "\n",
    "def find_nearby_pois(trip_points_gdf, pois_gdf, radius=500):\n",
    "    \"\"\"Find points of interest within radius (meters) of trip locations\"\"\"\n",
    "    trip_points_gdf['buffer'] = trip_points_gdf.geometry.buffer(radius)\n",
    "    joined = gpd.sjoin(\n",
    "        trip_points_gdf.set_geometry('buffer'),\n",
    "        pois_gdf,\n",
    "        how='left',\n",
    "        predicate='intersects'\n",
    "    )\n",
    "    return joined.groupby(level=0).size().rename('nearby_count')\n",
    "\n",
    "\n",
    "def process_partition(df_partition, businesses_gdf, attractions_gdf):\n",
    "    # Convert to GeoDataFrame for pickup locations\n",
    "    pickup_gdf = gpd.GeoDataFrame(\n",
    "        df_partition,\n",
    "        geometry=gpd.points_from_xy(\n",
    "            df_partition.pickup_longitude, \n",
    "            df_partition.pickup_latitude\n",
    "        ),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:32618\")\n",
    "    \n",
    "    # Find nearby businesses and attractions for pickup\n",
    "    df_partition['pickup_nearby_businesses'] = find_nearby_pois(pickup_gdf, businesses_gdf)\n",
    "    df_partition['pickup_nearby_attractions'] = find_nearby_pois(pickup_gdf, attractions_gdf)\n",
    "    \n",
    "    # Convert to GeoDataFrame for dropoff locations\n",
    "    dropoff_gdf = gpd.GeoDataFrame(\n",
    "        df_partition,\n",
    "        geometry=gpd.points_from_xy(\n",
    "            df_partition.dropoff_longitude, \n",
    "            df_partition.dropoff_latitude\n",
    "        ),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:32618\")\n",
    "    \n",
    "    # Find nearby businesses and attractions for dropoff\n",
    "    df_partition['dropoff_nearby_businesses'] = find_nearby_pois(dropoff_gdf, businesses_gdf)\n",
    "    df_partition['dropoff_nearby_attractions'] = find_nearby_pois(dropoff_gdf, attractions_gdf)\n",
    "    # fill na only these columns\n",
    "    df_partition['pickup_nearby_businesses'] = df_partition['pickup_nearby_businesses'].fillna(0)\n",
    "    df_partition['pickup_nearby_attractions'] = df_partition['pickup_nearby_attractions'].fillna(0)\n",
    "    df_partition['dropoff_nearby_businesses'] = df_partition['dropoff_nearby_businesses'].fillna(0)\n",
    "    df_partition['dropoff_nearby_attractions'] = df_partition['dropoff_nearby_attractions'].fillna(0)\n",
    "    \n",
    "    return df_partition\n",
    "\n",
    "\n",
    "meta_yellow['pickup_nearby_businesses'] = pd.Series(dtype='int64')\n",
    "meta_yellow['pickup_nearby_attractions'] = pd.Series(dtype='int64')\n",
    "meta_yellow['dropoff_nearby_businesses'] = pd.Series(dtype='int64')\n",
    "meta_yellow['dropoff_nearby_attractions'] = pd.Series(dtype='int64')\n",
    "meta_green = green_taxi_df._meta.copy()\n",
    "meta_green['pickup_nearby_businesses'] = pd.Series(dtype='int64')\n",
    "meta_green['pickup_nearby_attractions'] = pd.Series(dtype='int64')\n",
    "meta_green['dropoff_nearby_businesses'] = pd.Series(dtype='int64')\n",
    "meta_green['dropoff_nearby_attractions'] = pd.Series(dtype='int64')\n",
    "meta_fhv = fhv_df._meta.copy()\n",
    "meta_fhv['pickup_nearby_businesses'] = pd.Series(dtype='int64')\n",
    "meta_fhv['pickup_nearby_attractions'] = pd.Series(dtype='int64')\n",
    "meta_fhv['dropoff_nearby_businesses'] = pd.Series(dtype='int64')\n",
    "meta_fhv['dropoff_nearby_attractions'] = pd.Series(dtype='int64')\n",
    "meta_fhvgh = fhvhv_df._meta.copy()\n",
    "meta_fhvgh['pickup_nearby_businesses'] = pd.Series(dtype='int64')\n",
    "meta_fhvgh['pickup_nearby_attractions'] = pd.Series(dtype='int64')\n",
    "meta_fhvgh['dropoff_nearby_businesses'] = pd.Series(dtype='int64')\n",
    "meta_fhvgh['dropoff_nearby_attractions'] = pd.Series(dtype='int64')  \n",
    "\n",
    "\n",
    "yellow_taxi_df_location_enriched = yellow_taxi_df.map_partitions(\n",
    "    process_partition,\n",
    "    businesses_gdf=businesses_gdf,\n",
    "    attractions_gdf=attractions_gdf,\n",
    "    meta=meta_yellow\n",
    ")\n",
    "green_taxi_df_location_enriched = green_taxi_df.map_partitions(\n",
    "    process_partition,\n",
    "    businesses_gdf=businesses_gdf,\n",
    "    attractions_gdf=attractions_gdf,\n",
    "    meta=meta_green\n",
    ")\n",
    "fhvhv_df_location_enriched = fhvhv_df.map_partitions(\n",
    "    process_partition,\n",
    "    businesses_gdf=businesses_gdf,\n",
    "    attractions_gdf=attractions_gdf,\n",
    "    meta=meta_fhvgh,\n",
    ")\n",
    "fhv_df_location_enriched = fhv_df.map_partitions(\n",
    "    process_partition,\n",
    "    businesses_gdf=businesses_gdf,\n",
    "    attractions_gdf=attractions_gdf,\n",
    "    meta=meta_fhv\n",
    ")  \n",
    "\n",
    "\n",
    "yellow_taxi_df_location_enriched.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/YELLOW/augmented_with_pois\",\n",
    "    write_index=False\n",
    ")\n",
    "green_taxi_df_location_enriched.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/GREEN/augmented_with_pois\",\n",
    "    write_index=False\n",
    ")\n",
    "fhvhv_df_location_enriched.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/FHVH/augmented_with_pois\",\n",
    "    write_index=False\n",
    ")\n",
    "fhv_df_location_enriched.to_parquet(\n",
    "    f\"{OUTPUT_DIR}/FHV/augmented_with_pois\",\n",
    "    write_index=False\n",
    ")   "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
